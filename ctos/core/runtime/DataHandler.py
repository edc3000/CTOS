from Config import ACCESS_KEY, SECRET_KEY, PASSPHRASE, HOST_IP, HOST_USER, HOST_PASSWD, HOST_IP_1
import mysql.connector
from mysql.connector import Error
from datetime import datetime, timedelta, date
import pandas as pd
import os
from tqdm import tqdm
import requests
import zipfile
import time
import random
from util import base_url, rate_price2order, json
from collections import defaultdict
from mysql.connector.errors import DatabaseError

# å…¼å®¹ä¸åŒåˆ—åçš„å­—å…¸æ˜ å°„
COLUMN_MAPPING = {
    'trade_date': 'trade_date',
    'Open': 'open',
    'High': 'high',
    'Low': 'low',
    'Close': 'close',
    'vol1': 'vol1',
    'vol': 'vol',
}


class DataHandler:
    def __init__(self, host, database, user, password):
        self.conn = None
        try:
            self.conn = mysql.connector.connect(
                host=host,
                database=database,
                user=user,
                password=password
            )
            if self.conn.is_connected():
                print('__init__ DataHandler success~~~')

        except Error as e:
            print(e, '1111111111111')

    def create_table_if_not_exists(self, cursor, table_name):
        # 20250602 1730 è¿™é‡Œéœ€è¦è€ƒè™‘ä¸€ä¸ªäº‹æƒ…ï¼Œé‚£å°±æ˜¯shibè¿™ç§å‚»é€¼å¸ç§ï¼Œä»·é’±å·¨ä½ï¼Œäº¤æ˜“é‡å·¨å¤§ï¼Œç‹—æ—¥çš„ç›´æ¥è¶…æ¨¡äº†ã€‚
        # -- ä¿®æ”¹ SHIBUSDT_1d è¡¨
        # ALTER TABLE SHIBUSDT_1d
        # MODIFY vol1 DECIMAL(30,10),
        # MODIFY vol DECIMAL(30,10);

        create_table_query = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            trade_date DATETIME PRIMARY KEY,
            open DECIMAL(25, 10),
            high DECIMAL(25, 10),
            low DECIMAL(25, 10),
            close DECIMAL(25, 10),
            vol1 DECIMAL(25, 10),
            vol DECIMAL(25, 10)
        );
        """
        try:
            cursor.execute(create_table_query)
            print(f"Table {table_name} created successfully.")
        except Error as e:
            print(f"Failed to create table {table_name}: {e}")

    def insert_data(self, symbol, interval, data, remove_duplicates=False):
        table_name = f"{symbol.replace('-', '_')}_{interval}"
        try:
            if self.conn.is_connected():
                cursor = self.conn.cursor()
                # Ensure the table exists
                # self.create_table_if_not_exists(cursor, table_name)

                query = f"""INSERT INTO {table_name}
                            (trade_date, open, high, low, close, vol1, vol)
                            VALUES (%s, %s, %s, %s, %s, %s, %s)
                            ON DUPLICATE KEY UPDATE
                            open = VALUES(open), high = VALUES(high), low = VALUES(low),
                            close = VALUES(close), vol1 = VALUES(vol1), vol = VALUES(vol);"""
                # print('aaaaaa', data.iloc[0,0:10])
                data['vol1'] = data['vol1'] / 1e6
                formatted_data = [
                    (
                        parse_trade_date(row['trade_date']),  # Assume there's a function to parse trade_date correctly
                        row['open'],
                        row['high'],
                        row['low'],
                        row['close'],
                        row['vol1'],
                        row['vol']
                    )
                    for index, row in data.iterrows()
                ]

                cursor.executemany(query, formatted_data)
                self.conn.commit()
                print(cursor.rowcount, "records inserted into", table_name)
                if remove_duplicates:
                    self.remove_duplicates(table_name)
            else:
                print('æ²¡è¿ä¸Šï¼Ÿå’‹å›äº‹ï¼Ÿ')
        except Error as e:
            print(e, '222222222')

    def remove_duplicates(self, table_name):
        try:
            with self.conn.cursor() as cur:
                cur.execute(f"CREATE TEMPORARY TABLE keep_dates AS "
                            f"SELECT MIN(trade_date) AS trade_date FROM {table_name} GROUP BY trade_date")

                # åˆ é™¤ä¸åœ¨ keep_dates é‡Œçš„è¡Œï¼ˆå³åŒæ—¥æœŸçš„é‡å¤è¡Œï¼‰
                cur.execute(f"""
                    DELETE t FROM {table_name} t
                    LEFT JOIN keep_dates k USING (trade_date)
                    WHERE k.trade_date IS NULL;
                """)
                self.conn.commit()
                cur.execute("DROP TEMPORARY TABLE keep_dates")
                print(f"Duplicates removed in table {table_name}.")
        except Error as e:
            print("Error removing duplicates:", e)


    def fetch_data(self, symbol, interval, *args):
        """
        Enhanced fetch function to handle different data retrieval scenarios.
        - If called with one argument: last_X_data -> fetches the last X data points.
        - If called with two arguments: start_date, X_data_after -> fetches X data points after start_date.
        - If called with two arguments: end_date, X_data_before -> fetches X data points before end_date.
        """
        table_name = f"{symbol.replace('-', '_')}_{interval}"
        safe_table_name = table_name

        if len(args) == 1 and isinstance(args[0], int):
            query = f"SELECT * FROM {safe_table_name} ORDER BY trade_date DESC LIMIT %s"
            params = (args[0],)

        elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], int):
            if '-' in args[0]:  # èµ·å§‹æ—¥æœŸ + æ¡æ•°
                query = f"""SELECT * FROM {safe_table_name}
                            WHERE trade_date >= %s
                            ORDER BY trade_date ASC
                            LIMIT %s"""
            else:  # ç»“æŸæ—¥æœŸ + æ¡æ•°
                query = f"""SELECT * FROM {safe_table_name}
                            WHERE trade_date <= %s
                            ORDER BY trade_date DESC
                            LIMIT %s"""
            params = (args[0], args[1])

        elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], str):
            query = f"""SELECT * FROM {safe_table_name}
                        WHERE trade_date BETWEEN %s AND %s"""
            params = (args[0], args[1])


        # query = ""
        # params = ()
        # if len(args) == 1 and isinstance(args[0], int):
        #     # Last X data points
        #     query = f"SELECT * FROM {table_name} ORDER BY trade_date DESC LIMIT %s"
        #     params = (args[0],)
        # elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], int):
        #     # X data after start_date or before end_date based on date format
        #     if '-' in args[0]:  # likely a date string
        #         query = f"SELECT * FROM {table_name} WHERE trade_date >= '%s' ORDER BY trade_date ASC LIMIT %s"
        #     else:
        #         query = f"SELECT * FROM {table_name} WHERE trade_date <= '%s' ORDER BY trade_date DESC LIMIT %s"
        #     params = (args[0], args[1])
        # elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], str):
        #     # X data after start_date or before end_date based on date format
        #     if '-' in args[0] and '-' in args[1]:  # likely a date string
        #         query = f"SELECT * FROM {table_name} WHERE trade_date >= '%s' AND trade_date <= %s"
        #     params = (args[0], args[1])
        #
        try:
            if self.conn.is_connected():
                cursor = self.conn.cursor(dictionary=True)
                cursor.execute(query, params)
                result = cursor.fetchall()
                df = pd.DataFrame(result)
                if 'DESC' in query:  # If the query was in descending order, reverse the DataFrame
                    df = df.iloc[::-1].reset_index(drop=True)
                return df
        except Error as e:
            print(f"Error fetching data: {e}")
            return pd.DataFrame()  # Return an empty DataFrame in case of error


    def close(self):
        if self.conn is not None and self.conn.is_connected():
            self.conn.close()
            print('Database connection closed.')

    def check_missing_days(self,
                           start_date=None,
                           coins=None,
                           intervals=None):
        """
        æ‰«ææ•°æ®åº“ä¸­ [start_date, æ˜¨å¤©] åŒºé—´çš„ç¼ºå¤±äº¤æ˜“æ—¥
        --------------------------------------------------
        :param data_handler: åˆå§‹åŒ–å®Œæˆçš„ DataHandler å®ä¾‹
        :param start_date:   èµ·å§‹æ—¥æœŸ (str|datetime)ï¼Œé»˜è®¤ '2017-01-01'
        :param coins:        å¸ç§åˆ—è¡¨ï¼Œé»˜è®¤ rate_price2order çš„é”®ï¼ˆå»æ‰ 'ip'ï¼‰
        :param intervals:    K çº¿å‘¨æœŸåˆ—è¡¨ï¼Œé»˜è®¤ ['1m','15m','30m','1h','4h','1d']
        :return: dict{coin â†’ dict{interval â†’ list[ç¼ºå¤±æ—¥æœŸ(date)]}}
        """
        # âš‘ é»˜è®¤é…ç½®
        if intervals is None:
            intervals = ['1m', '15m', '30m', '1h', '4h', '1d']
        if coins is None:
            coins = [x for x in rate_price2order.keys() if x != 'ip']

        missing_map = {}

        for cc in coins:
            if not start_date:
                start_date = find_start_date(base_url, cc.upper() + 'USDT', '1d')
            start_dt = pd.to_datetime(start_date)
            end_dt = datetime.utcnow().date() - timedelta(days=1)  # æ˜¨å¤©

            coin = cc.upper() + 'USDT'
            for interval in intervals:
                try:
                    # â‘  ä¸€æ¬¡æ€§æ‹‰å–æ—¥æœŸåˆ—
                    df = self.fetch_data(
                        coin, interval,
                        start_dt.strftime("%Y-%m-%d"),  # æ— éœ€æ—¶åˆ†ç§’
                        end_dt.strftime("%Y-%m-%d 23:59:59")
                    )
                    if df.empty:
                        # æ•°æ®å…¨ç¼ºï¼Œç›´æ¥è®°å½•æ•´æ®µ
                        exp_days = pd.date_range(start_dt, end_dt, freq='D').date
                        missing_map.setdefault(coin, {})[interval] = list(exp_days)
                        print(f"[ç©ºè¡¨] {coin}-{interval} ç¼ºå¤± {len(exp_days)} å¤©")
                        continue

                    # â‘¡ ç°æœ‰æ—¥æœŸé›†åˆ
                    df['trade_date'] = pd.to_datetime(df['trade_date'], unit='ms')
                    present_days = set(df['trade_date'].dt.date.unique())

                    # â‘¢ æœŸæœ›æ—¥æœŸé›†åˆ
                    expected_days = pd.date_range(start_dt, end_dt, freq='D').date
                    missing_days = sorted(set(expected_days) - present_days)

                    if missing_days:  # ä»…è®°å½•ç¼ºå¤±
                        missing_map.setdefault(coin, {})[interval] = missing_days
                        print(f"[ç¼ºå¤±] {coin}-{interval}: {len(missing_days)} å¤©")
                except Exception as e:
                    print(f"æ£€æŸ¥å¤±è´¥ {coin}-{interval}: {e}")
            start_date = None
        return missing_map



def fetch_kline_data(exchange, interval, limit, symbol):
    """
    è·å–æŒ‡å®šäº¤æ˜“å¯¹å’Œæ—¶é—´æ®µçš„Kçº¿æ•°æ®ã€‚
    :param exchange: OkexSpot å®ä¾‹ã€‚
    :param interval: Kçº¿å›¾çš„æ—¶é—´é—´éš”ã€‚
    :param limit: è¿”å›çš„æ•°æ®æ•°é‡ã€‚
    :param symbol: äº¤æ˜“å¯¹ï¼Œå¦‚ 'ETH-USDT'ã€‚
    :return: Kçº¿æ•°æ®çš„DataFrameã€‚
    """
    df, _ = exchange.get_kline(interval, limit, symbol)
    return df


def check_data_exists(base_url, symbol, interval, date):
    date_str = date.strftime('%Y-%m-%d')
    filename = f"{symbol}-{interval}-{date_str}.zip"
    url = f"{base_url}/{symbol}/{interval}/{filename}"
    response = requests.get(url)
    return response.status_code == 200


# â”€â”€ ç¼“å­˜æ–‡ä»¶ä½ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE_PATH = os.path.expanduser("~/Quantify/okx/trade_log_okex")
CACHE_FILE = os.path.join(CACHE_PATH, "start_date_cache.json")

def _load_cache():
    if not os.path.exists(CACHE_FILE):
        return {}
    try:
        with open(CACHE_FILE, "r") as f:
            return json.load(f)
    except Exception:
        return {}        # è¯»å¤±è´¥åˆ™è§†ä¸ºæ— ç¼“å­˜

def _save_cache(cache):
    os.makedirs(CACHE_PATH, exist_ok=True)
    with open(CACHE_FILE, "w") as f:
        json.dump(cache, f, default=str, indent=2)

def find_start_date(base_url, symbol, interval, earliest_date=datetime(2015, 1, 1), latest_date=datetime.now()):
    key = f"{symbol}_{interval}"
    cache = _load_cache()

    # â‘  å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if key in cache:
        cached_val = datetime.fromisoformat(cache[key])
        print(f"âš¡ ç¼“å­˜å‘½ä¸­ï¼š{symbol}-{interval} -> {cached_val.date()}")
        return cached_val

    # â‘¡ å¦åˆ™æ‰§è¡ŒåŸé€»è¾‘ï¼ˆç½‘ç»œäºŒåˆ†æŸ¥æ‰¾ï¼‰
    print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ {symbol} - {interval} æœ€æ—©çš„æ•°æ®èµ·å§‹æ—¶é—´...")
    left, right, result = earliest_date, latest_date, None

    while left <= right:
        mid = left + (right - left) // 2
        exists = check_data_exists(base_url, symbol, interval, mid)
        print(f"æ£€æŸ¥ {mid.strftime('%Y-%m-%d')} : {'å­˜åœ¨âœ…' if exists else 'ä¸å­˜åœ¨âŒ'}")

        if exists:
            result = mid
            right = mid - timedelta(days=1)
        else:
            left = mid + timedelta(days=1)

    print(f"ğŸ“Œ æœ€æ—©çš„æ•°æ®èµ·å§‹æ—¶é—´æ˜¯ï¼š{result if result else 'æœªæ‰¾åˆ°'}")

    # â‘¢ å†™å…¥ç¼“å­˜ï¼ˆä»…åœ¨æ‰¾åˆ°ç»“æœæ—¶ï¼‰
    if result:
        cache[key] = result.isoformat()
        _save_cache(cache)

    return result


def download_and_process_binance_data(base_url, symbol, start_date, end_date, intervals, missing_days=None):
    """
    Download and process Binance k-line data from the specified URL.
    """
    # 1âƒ£ é¢„ç”Ÿæˆå¾…å¤„ç†æ—¥æœŸåˆ—è¡¨
    if missing_days is None:
        all_days = pd.date_range(start_date.date(), end_date.date() - timedelta(days=1), freq='D').date
    else:
        all_days = sorted(missing_days)   # è½¬æˆ list å¹¶æ’åºï¼Œä¾¿äº tqdm

    # 2âƒ£ éå† interval ä¸æ—¥æœŸ
    for interval in intervals:
        for day in tqdm(all_days, desc=f"download_and_process_binance_data {symbol}-{interval}"):
            # è‹¥ missing_days=None åˆ™èµ°å…¨é‡ï¼›è‹¥é None ä¸” day ä¸åœ¨é›†åˆï¼Œä¹Ÿä¸ä¼šè¿›æ¥
            date_str = day.strftime('%Y-%m-%d')
            filename = f"{symbol}-{interval}-{date_str}.zip"
            csv_filename = f"{symbol}-{interval}-{date_str}.csv"
            target_csv_path = os.path.join('data/{}'.format(interval), csv_filename)
            # Check if the file already exists to avoid re-downloading
            IS_DOWNLOAD = False
            if not os.path.exists(target_csv_path):
                # print('\r{} - {} --> {}'.format(interval, current_date, end_date), end='')
                time.sleep(0.1 + random.randint(0, 20) / 20)
                # Construct the URL and download the file
                url = f"{base_url}/{symbol}/{interval}/{filename}"
                response = requests.get(url)
                if response.status_code == 200:
                    # Save the zip file temporarily
                    zip_path = os.path.join('data/{}'.format(interval), filename)
                    with open(zip_path, 'wb') as f:
                        f.write(response.content)

                    # Extract the zip file
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall('data/{}'.format(interval))

                    # Rename and move the extracted CSV file
                    extracted_file = os.path.join('data/{}'.format(interval), csv_filename.replace('.csv',
                                                                               '.csv'))  # Assuming the extracted file has a predictable name
                    os.rename(extracted_file, target_csv_path)

                    # Remove the zip file after extraction
                    os.remove(zip_path)
                    IS_DOWNLOAD = True
                elif response.status_code == 404:
                    time.sleep(0.1)
                    continue
                else:
                    time.sleep(0.2)
                    print(f"Failed to download data for {date_str}: Status code {response.status_code}")
            # Read, process, and save the CSV data
            if os.path.exists(target_csv_path) and IS_DOWNLOAD:
                df = pd.read_csv(target_csv_path, header=None,
                                 names=["Open time", "Open", "High", "Low", "Close", "Volume", "Close time",
                                        "Quote asset volume", "Number of trades", "Taker buy base asset volume",
                                        "Taker buy quote asset volume", "Ignore"])
                try:
                    ### 2025å¹´å¼€å§‹çš„æ•°æ®é‡‡ç”¨äº†æ›´ç»†ç²’åº¦çš„æ—¶é—´ï¼Œä¸€ç›´æ— æ³•è½¬æ¢ï¼Œæ°”äºº
                    open_time = pd.to_numeric(df['Open time'], errors='coerce')
                    # å¦‚æœæ—¶é—´æˆ³å¤ªå¤§ï¼Œå°è¯•é™¤ä»¥ 1000 æˆ– 1000000 ç¼©å°åˆ°æ¯«ç§’çº§
                    if open_time.max() > 1e13:
                        open_time = open_time // 1000  # è½¬æ¢ä¸ºæ¯«ç§’çº§
                    df['trade_date'] = pd.to_datetime(open_time, unit='ms')
                    # df['trade_date'] = pd.to_datetime(df['Open time'], unit='ms')
                    df['vol1'] = df['Quote asset volume']
                    df['vol'] = df['Volume']
                    df = df[['trade_date', 'Open', 'High', 'Low', 'Close', 'vol1', 'vol']]
                    df.columns = df.columns.str.lower()
                    df.to_csv(target_csv_path, index=False)
                except Exception as e:
                    print('\n', e, '\n', target_csv_path, '\n333333333333', '\n', df)
                    if str(e).find('Out of b') != -1:
                        break


def parse_trade_date(trade_date):
    """
    æ¥æ”¶ Timestamp / datetime / int(ms|s) / strï¼Œç»Ÿä¸€è¿”å› '%Y-%m-%d %H:%M:%S'
    """
    # â‘  Timestamp æˆ– datetime
    if isinstance(trade_date, (pd.Timestamp, datetime)):
        return trade_date.strftime('%Y-%m-%d %H:%M:%S')

    # â‘¡ çº¯æ•°å­—ï¼šæ¯«ç§’çº§æˆ–ç§’çº§
    if isinstance(trade_date, (int, float)):
        # ç²—åˆ¤ï¼š10 ä½â‰ˆç§’ï¼Œ13 ä½â‰ˆæ¯«ç§’
        seconds = trade_date / 1000 if trade_date > 1e11 else trade_date
        return datetime.utcfromtimestamp(seconds).strftime('%Y-%m-%d %H:%M:%S')

    # â‘¢ å­—ç¬¦ä¸² â†’ å°è¯•è§£æ
    try:
        ts = pd.to_datetime(trade_date, errors='raise')
        return ts.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        raise ValueError(f"æ— æ³•è§£æ trade_date={trade_date}: {e}")



# 20250602 1500 ä¿®æ”¹å®Œæ¯• å…ˆæ£€æŸ¥ç°æœ‰æ•°æ®åº“çš„æ•°æ®ï¼Œç„¶åæŸ¥æ¼è¡¥ç¼ºå»æœç´¢æ•°æ®
# 20250602 1500 è¿˜å¢åŠ äº†check_csv_format.pyæ¥æ•´ä½“çš„æ£€æŸ¥ä¸€éæ•°æ®çš„ä¿å­˜æ ¼å¼
def get_all_binance_data(symbol_now='ETHUSDT', missing_days=None):
    # æ„Ÿæ©è®©æˆ‘å‘ç°æ•°æ®ï¼š https: // bmzhp.com / blockchain / 396
    # Usage example
    symbol = symbol_now
    start_date = find_start_date(base_url, symbol, '1d')
    # start_date = datetime(2020, 5, 1)
    end_date = datetime.now()
    intervals = time_gaps
    download_and_process_binance_data(base_url, symbol, start_date, end_date, intervals, missing_days)


def read_processed_data(symbol, interval, start_date, end_date, missing_days=None):
    """
    Reads processed trading data for a given symbol and interval within a specified date range.

    :param symbol: The trading symbol, e.g., 'ETHUSDT'
    :param interval: The data interval, e.g., '1m', '15m', '30m', '1h', '4h', '1d'
    :param start_date: The start date as a string in 'YYYY-MM-DD' format
    :param end_date: The end date as a string in 'YYYY-MM-DD' format
    :return: A pandas DataFrame containing the requested data
    """
    # Convert start and end dates to datetime objects
    # 0âƒ£ ç»Ÿä¸€æ—¥æœŸç±»å‹
    start_date = pd.to_datetime(start_date).date()
    end_date   = pd.to_datetime(end_date).date()

    # 1âƒ£ ç”Ÿæˆå¾…è¯»å–æ—¥æœŸåˆ—è¡¨
    if missing_days is None:
        dates_to_read = pd.date_range(start_date, end_date - timedelta(days=1), freq='D').date
    else:
        # åªä¿ç•™è½åœ¨ [start_date, end_date) åŒºé—´å†…çš„ç¼ºå¤±æ—¥æœŸï¼Œé¿å…è¶Šç•Œ
        dates_to_read = sorted(d for d in missing_days if start_date <= d < end_date)

    # 2âƒ£ éå†å¹¶è¯»æ–‡ä»¶
    data_folder = f"data/{interval}"
    all_data = []

    for day in dates_to_read:
        date_str  = day.strftime('%Y-%m-%d')
        filename  = f"{symbol}-{interval}-{date_str}.csv"
        file_path = os.path.join(data_folder, filename)

        if os.path.exists(file_path):
            df = pd.read_csv(file_path, parse_dates=['trade_date'])  # â¬…ï¸ ä¸€è¡Œæå®š
            df.columns = df.columns.str.lower()
            all_data.append(df)
        else:
            print(f"âš ï¸  æ–‡ä»¶ç¼ºå¤±: {file_path}")

    # 3âƒ£ åˆå¹¶è¿”å›
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        return combined_df
    else:
        return pd.DataFrame()      # æ²¡æ‰¾åˆ°ä»»ä½•æ•°æ®


# 20250602 1500  å°†æ—¶é—´æ®µæ€§è´¨çš„æ’å…¥ï¼Œè½¬å˜ä¸ºç¦»æ•£æ—¶é—´åºåˆ—çš„æ’å…¥
def batch_insert_data(data_handler, symbol, interval, df, batch_size=1000, missing_days=None):
    # 0âƒ£ è¿‡æ»¤ç¼ºå¤±æ—¥
    if missing_days is not None and not df.empty:
        df = df[df['trade_date'].dt.date.isin(missing_days)]
        if df.empty:
            print(f"\r[{symbol}-{interval}] æ— éœ€æ’å…¥ï¼ˆç¼ºå¤±æ—¥å·²å…¨éƒ¨è¡¥é½ï¼‰", end='')
            return


    # 1âƒ£ åˆ†æ‰¹æ’å…¥
    for start in tqdm(range(0, len(df), batch_size), desc=f"batch_insert_data {symbol}-{interval} insert"):
        end = start + batch_size
        batch_df = df.iloc[start:end]               # iloc æ›´ç¨³å¦¥
        data_handler.insert_data(symbol, interval, batch_df)
        print(f"Inserted batch rows {start} ~ {end - 1}")

    # 2âƒ£ å»é‡
    table_name = f"{symbol.replace('-', '_')}_{interval}"
    data_handler.remove_duplicates(table_name)




# 20250602 1500  éœ€è¦æ”¯æŒé’ˆå¯¹ç‰¹å®šçš„æ—¥æœŸå»æ’å…¥ï¼ŒæŒ‡å®šæ—¥æœŸé›†åˆï¼Œç”±å‰é¢çš„è·å–æ•°æ®å‡½æ•°æä¾›åˆ°å…¨å±€å˜é‡ä¸­
def insert_binance_data_into_mysql(data_handler, symbol_now='ETHUSDT', missing_days=None):
    symbol     = symbol_now.upper()
    start_date = find_start_date(base_url, symbol, '1d')                     # å¯æŒ‰éœ€æ”¹æˆåŠ¨æ€æŸ¥æ‰¾
    end_date   = datetime.now()

    for interval in tqdm(time_gaps, desc=f"insert_binance_data_into_mysql {symbol} loop"):
        # â‘  è¯»å–æœ¬åœ°å·²å¤„ç†å¥½çš„ CSV / Parquet
        df = read_processed_data(symbol, interval, start_date, end_date, missing_days)

        if df.empty:
            print(f"[{symbol}-{interval}] æ— æ•°æ®å¯è¯»")
            continue

        # print(df.head(), '\n', df.tail(), '\n', len(df))

        # â‘¡ æ‰¹é‡å†™å…¥ï¼Œå¸¦ missing_days è¿‡æ»¤
        batch_insert_data(
            data_handler=data_handler,
            symbol=symbol,
            interval=interval,
            df=df,
            missing_days=missing_days              # â¬…ï¸ æ–°å¢
        )


def export_daily_data(data_handler, base_path="~/Quantify/okx/data"):
    """
    æŒ‰å¤©å¯¼å‡ºKçº¿æ•°æ®åˆ°CSVæ–‡ä»¶
    :param data_handler: å·²åˆå§‹åŒ–çš„DataHandlerå®ä¾‹
    :param base_path: åŸºç¡€å­˜å‚¨è·¯å¾„ï¼ˆé»˜è®¤ ~/Quantify/okx/dataï¼‰
    """
    # é…ç½®å‚æ•°
    time_gaps = ['1m', '15m', '30m', '1h', '4h', '1d']
    coins = [x for x in list(rate_price2order.keys()) if x != 'ip']  # æ›¿æ¢ä¸ºä½ çš„å¸ç§åˆ—è¡¨

    for cc in coins:
        for interval in time_gaps:
            # è·å–è¡¨åå¯¹åº”çš„æ‰€æœ‰æ—¥æœŸ
            try:
                coin = cc.upper()+'USDT'
                # æŸ¥è¯¢æ‰€æœ‰æ•°æ®ç‚¹ï¼ˆä»…è·å–æ—¥æœŸåˆ—ï¼‰
                df_all = data_handler.fetch_data(coin, interval, '2017-01-01', '2025-05-03')  # å‡è®¾è¶³å¤Ÿå¤§çš„æ•°è·å–å…¨éƒ¨æ•°æ®
                if df_all.empty:
                    print(f"æ— æ•°æ®å¯å¯¼å‡º: {coin}_{interval}")
                    continue

                # è½¬æ¢æ—¥æœŸåˆ—ä¸ºæ—¥æœŸå¯¹è±¡
                df_all['trade_date'] = pd.to_datetime(df_all['trade_date'])
                # æå–å”¯ä¸€æ—¥æœŸï¼ˆæŒ‰å¤©åˆ†ç»„ï¼‰
                unique_dates = df_all['trade_date'].dt.date.unique()

                # æŒ‰å¤©å¯¼å‡º
                for date in unique_dates:
                    # æ„é€ æ—¶é—´èŒƒå›´
                    start_time = datetime.combine(date, datetime.min.time())
                    end_time = start_time + timedelta(days=1) - timedelta(seconds=1)

                    # è·å–å½“å¤©æ•°æ®
                    df_day = data_handler.fetch_data(
                        coin, interval,
                        start_time.strftime("%Y-%m-%d %H:%M:%S"),
                        end_time.strftime("%Y-%m-%d %H:%M:%S")
                    )
                    if df_day.empty:
                        continue

                    # åˆ›å»ºç›®å½•
                    save_dir = os.path.expanduser(os.path.join(base_path, interval))
                    os.makedirs(save_dir, exist_ok=True)

                    # ç”Ÿæˆæ–‡ä»¶å
                    filename = f"{coin}-{interval}-{date.strftime('%Y-%m-%d')}.csv"
                    filepath = os.path.join(save_dir, filename)

                    if os.path.exists(filepath):
                        print(f"\r å·²å­˜åœ¨: {filepath}", end='')
                    else:
                        # ä¿å­˜CSV
                        df_day.to_csv(filepath, index=False)
                        print(f"\rå·²ä¿å­˜: {filepath}" ,end='')

            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {coin}_{interval}: {str(e)}")
                continue



# ç§’æ•°æ­¥é•¿æ˜ å°„
STEP_SEC = {
    '1m': 60,  '5m': 300,  '15m': 900,
    '30m': 1800,  '1h': 3600,  '4h': 14400,
    '1d': 86400
}

def check_and_repair_tables(data_handler, coins, time_gaps):
    """
    å¯¹æ¯å¼ è¡¨åšâ€œé€æ—¶é—´æˆ³æ‰«æâ€ï¼Œå‘ç°ç¼ºå£ => æŠŠä¸Šä¸€æ¡è®°å½•å¤åˆ¶æ’å…¥
    """
    conn = data_handler.conn
    cur  = conn.cursor(dictionary=True)

    for coin in coins:
        symbol = f"{coin.upper()}USDT"
        for iv in time_gaps:
            step = STEP_SEC[iv]
            table = f"{symbol}_{iv}"

            # 0âƒ£ æœ€æ—©ã€æœ€æ™šæ—¶é—´
            cur.execute(f"SELECT MIN(trade_date) AS min_dt, MAX(trade_date) AS max_dt FROM {table}")
            row = cur.fetchone()
            if not row['min_dt']:
                print(f"[ç©ºè¡¨] {table} è·³è¿‡")
                continue
            t_min, t_max = row['min_dt'], row['max_dt']
            print(f"\nğŸ” {table} æ‰«æ {t_min} â†’ {t_max}")

            # 1âƒ£ é¢„ç¼–è¯‘ SQL
            exist_sql = f"SELECT 1 FROM {table} WHERE trade_date = %s LIMIT 1"
            insert_sql = (
                f"INSERT INTO {table} "
                f"(trade_date, open, high, low, close, vol1, vol)"   # æŒ‰å®é™…åˆ—æ”¹
                f"SELECT %s, open, high, low, close, vol1, vol "
                f"FROM {table} WHERE trade_date = %s LIMIT 1"
            )

            # query = f"""INSERT INTO {table_name}
            #                             (trade_date, open, high, low, close, vol1, vol)
            #                             VALUES (%s, %s, %s, %s, %s, %s, %s)
            #                             ON DUPLICATE KEY UPDATE
            #                             open = VALUES(open), high = VALUES(high), low = VALUES(low),
            #                             close = VALUES(close), vol1 = VALUES(vol1), vol = VALUES(vol);"""

            t_cur   = t_min
            inserted, checked = 0, 0

            while t_cur < t_max:
                t_next = t_cur + timedelta(seconds=step)
                cur.execute(exist_sql, (t_next.strftime("%Y-%m-%d %H:%M:%S"),))
                exists = cur.fetchone()
                checked += 1
                print(f'\r {t_cur}', end='')
                if not exists:
                    # å¤åˆ¶ä¸Šä¸€è¡Œæ’å…¥
                    cur.execute(insert_sql, (t_next.strftime("%Y-%m-%d %H:%M:%S"), t_cur.strftime("%Y-%m-%d %H:%M:%S")))
                    inserted += 1
                    print(f'\r æ£€æµ‹åˆ° {t_cur} ä¸å­˜åœ¨ï¼æ’è¡¥ä¸€æ¬¡ï¼', end='')
                    if inserted % 5000 == 0:      # æ‰¹é‡æäº¤
                        conn.commit()
                        print(f"   å·²ä¿®è¡¥ {inserted} æ¡ â€¦")

                t_cur = t_next

            conn.commit()
            print(f"âœ… {table} æ‰«æå®Œæˆï¼Œæ£€æŸ¥ {checked} æ­¥ï¼Œè¡¥ {inserted} è¡Œ")

    cur.close()
    print("\nğŸ‰ æ‰€æœ‰è¡¨ä¿®è¡¥å®Œæ¯•")

if __name__ == '__main__':
    from okex import OkexSpot
    # å‡è®¾exchangeæ˜¯OkexSpotçš„å®ä¾‹åŒ–å¯¹è±¡
    exchange = OkexSpot(
            symbol="ETH-USD-SWAP",
            access_key=ACCESS_KEY,
            secret_key=SECRET_KEY,
            passphrase=PASSPHRASE,
            host=None
        )
    # è°ƒç”¨fetch_kline_dataå‡½æ•°è·å–Kçº¿æ•°æ®
    # df_kline_1m = fetch_kline_data(exchange, '1m', 400, 'ETH-USD-SWAP')
    # df_kline_15m = fetch_kline_data(exchange, '15m', 400, 'ETH-USD-SWAP')
    # df_kline_30m = fetch_kline_data(exchange, '30m', 400, 'ETH-USD-SWAP')
    # df_kline_1h = fetch_kline_data(exchange, '1h', 400, 'ETH-USD-SWAP')
    # df_kline_4h = fetch_kline_data(exchange, '4h', 400, 'ETH-USD-SWAP')
    # df_kline_1d = fetch_kline_data(exchange, '1d', 400, 'ETH-USD-SWAP')

    # æ˜¾ç¤ºè·å–çš„æ•°æ®
    # print(df_kline_1m.head())

    # å‡è®¾data_handleræ˜¯DataHandlerçš„å®ä¾‹åŒ–å¯¹è±¡
    data_handler = DataHandler(HOST_IP_1, 'TradingData', HOST_USER, HOST_PASSWD)
    #
    # # å°†æ•°æ®æ’å…¥åˆ°æ•°æ®åº“ä¸­
    # data_handler.insert_data( 'ETH-USD-SWAP', '1m', df_kline_1m)
    # data_handler.insert_data( 'ETH-USD-SWAP', '15m', df_kline_15m)
    # data_handler.insert_data( 'ETH-USD-SWAP', '30m', df_kline_30m)
    # data_handler.insert_data( 'ETH-USD-SWAP', '1h', df_kline_1h)
    # data_handler.insert_data( 'ETH-USD-SWAP', '4h', df_kline_4h)
    # data_handler.insert_data( 'ETH-USD-SWAP', '1d', df_kline_1d)
    # 'xrp', 'bnb', 'sol', 'ada', 'doge', 'trx',

    # export_daily_data(data_handler)

    time_gaps = ['1m', '15m', '30m', '1h', '4h', '1d']
    # time_gaps = ['1m']
    coins = list(rate_price2order.keys())
    check_and_repair_tables(data_handler, coins, time_gaps)
    # for coin in [x for x in coins]:
    #     for interval in time_gaps:
    #         try:
    #             coin_name = coin.upper() + 'USDT'
    #             # data_handler.remove_duplicates(coin_name + '_' + interval)
    #             missing_days = data_handler.check_missing_days(coins=[coin], intervals=[interval])
    #             # print(missing_days)
    #             print('process coin:', coin_name, len(missing_days[coin_name][interval]))
    #             get_all_binance_data(coin_name, missing_days[coin_name][interval])
    #             insert_binance_data_into_mysql(data_handler, coin_name, missing_days[coin_name][interval])
    #             os.system(f'echo {coin_name}æ—¥çº¿å®Œæˆ {" ".join(time_gaps)} >> ä¸‹è½½å¸å®‰æ•°æ®.txt')
    #         except Exception as e:
    #             print('qqqq', e)
    data_handler.close()

#
# coins = ['btc', 'eth', 'xrp', 'bnb', 'sol', 'ada', 'doge', 'trx', 'ltc', 'shib', 'link', 'dot', 'om', 'apt', 'uni', 'hbar', 'ton', 'sui', 'avax', 'fil', 'ip', 'gala', 'sand']
#
# for c in coins:
#     tbl = f"{c.upper()}USDT_1d"
#     print(
#         f"SELECT COUNT(*) "
#         f"FROM {tbl} "
#         f"WHERE trade_date > '2024-01-01 00:00:00';"
#     )